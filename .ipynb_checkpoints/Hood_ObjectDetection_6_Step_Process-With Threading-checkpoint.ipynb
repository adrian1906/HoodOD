{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  \n",
    "\n",
    "This example program develops a HOG based object detector for things like faces, pedestrians, and any other semi-rigid object.  In particular, we go though the steps to train the kind of sliding window object detector first published by Dalal and Triggs in 2005 in the  paper Histograms of Oriented Gradients for Human Detection.\n",
    "\n",
    "It is similar to the method implemented in dlib (more optimized). However, this technique allows more control of the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the XML files\n",
    "DLIB requires images and bounding boxes around the labelled object. It has its own strructure for the XML files:\n",
    "\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<dataset>\n",
    "    <name>dataset containing bounding box labels on images</name>\n",
    "    <comment>created by BBTag</comment>\n",
    "    <tags>\n",
    "        <tag name=\"RunBib\" color=\"#032585\"/>\n",
    "    </tags>\n",
    "    <images>\n",
    "        <image file=\"B:/DataSets/2016_USATF_Sprint_TrainingDataset/_hsp3997.jpg\">\n",
    "            <box top=\"945\" left=\"887\" width=\"85\" height=\"53\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "            <box top=\"971\" left=\"43\" width=\"103\" height=\"56\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "            <box top=\"919\" left=\"533\" width=\"100\" height=\"56\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "        </image>\n",
    "        <image file=\"B:/DataSets/2016_USATF_Sprint_TrainingDataset/_hsp3989.jpg\">\n",
    "            <box top=\"878\" left=\"513\" width=\"111\" height=\"62\">\n",
    "                <label>my_label</label>\n",
    "            </box>\n",
    "        </image>     \n",
    "   </images>\n",
    "</dataset>\n",
    "top: Top left y value\n",
    "height: Height (positive down)\n",
    "left: Top left x value\n",
    "width: Width (positive to the right)\n",
    "\n",
    "To create your own XML files you can use the imglab tool which can be found in the tools/imglab folder.  It is a simple graphical tool for labeling objects in images with boxes.  To see how to use it read the tools/imglab/README.txt file.  But for this example, we just use the training.xml file included with dlib.\n",
    "\n",
    "Its a two part process to load the tagger.\n",
    "1.) typing the following command:\n",
    "#####    b:\\HoodMachineLearning\\dlib\\tools\\build\\Release\\imglab.exe -c mydataset.xml B:\\HoodMachineLearning\\datasets\\MyImage\n",
    "2.) \n",
    "####     b:\\HoodMachineLearning\\dlib\\tools\\build\\Release\\imglab.exe -c mydataset.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pyramids and sliding windows\n",
    "\n",
    "The technique uses image pyramids and sliding windows to minimize the effect of object location and object size. The pyramid is a set of subsample images and the sliding window remains the same and moves from left to right and top to bottom of each scale of the image.\n",
    "\n",
    "### Image Pyramids\n",
    "<img src=\"ImagePyramid.jpg\">\n",
    "\n",
    "Note: Dalai and Triggs showed that performance is reduced if you apply gaussian smoothing at each layer==> ski this stip\n",
    "\n",
    "### Sliding Window\n",
    "\n",
    "<img src=\"sliding_window_example.gif\" loop=3>\n",
    "\n",
    "* It is common to use a stepSize of 4 to 8 pixels\n",
    "* windowSize is the size of the Kernal. An object detector will work best if the aspect ratio of the kernal is close to that of the desired object. Note: The sliding window size is also important for the HOG filter. For the HOG filter two parameters are important: <b>pixels_per_cell</b> and <b>cells_per_block </b>\n",
    "\n",
    "In order to avoid having to 'guess' at the best window size that will satisfy object detector requirements and HOG requirments, a \"explore_dims.py\" method is used.\n",
    "\n",
    "1.) Meet object detection requirments: loads all the images and computes the average width, average height, and computes the aspect ratio from those values.\n",
    "2.) Meet HOG requirments: Pyimage rule of thumb is to divide the above values by two (ie, 1/4th the average size)\n",
    "    * This reduces the size of the HOG feature vector\n",
    "    * By dividing by two, a nice balance is struck between HOG feature vector size and reasonable window size.\n",
    "    * Note: Our sliding_window dimension needs to be divisible by pixels_per_cell and cells_per_block so that the HOG descriptor will 'fit' into the window size\n",
    "    * Its common for 'pixels_per_cell' to be a multiple of 4 and cells_per_block in the set (1,2,3)\n",
    "    * Start with pixels_per_cell=(4,4) and cells_per_block=(2,2)\n",
    "    * For example, in the Pyimage example, average W: 184 and average H:62. Divide by 2 ==> 92,31\n",
    "    * Find values close to 92,31 that are divisible by 4 (and 2): 96,32  (Easy)\n",
    "    * OBSERVATION:  When defining the binding boxes, it is best if all are around the same size. This can be difficult.  \n",
    "\n",
    "### The 6 Step Framework\n",
    "1. Sample P positive samples for your training data of the objects you want to detect. Extract HOG features from these objects.\n",
    "    * If given an a general image containing the object, bounding boxes will also need to be given that indicate the location of the image\n",
    "2. Sample N negative samples that do not contain the object and extract HOG features. In general N>>P  (I'd suggest images similar in size and aspect ratio to the P samples. I'd also avoid the bounding boxes and make the entire image the negative image. Pyimagesearch recommends using the 13 Natural Scene Category of the vision.stanford.edu/resources_links.html page\n",
    "3. Train a Linear Support Vector Machine (SVM) on the negative images (class 0) and positive image (class 1)\n",
    "4. Hard Negative Mining - for the N negative images, apply Sliding window and test the classifier. Ideally, they should all return 0. If they return a 1 indicating an incorrect classification, add it to the training set (for the next round of re-training)\n",
    "5. Re-train classifier using with the added images from Hard Negative Mining (Usually once is enough)\n",
    "6. Apply against test dataset, define a box around regions of high probability, when finished with the image, find the boxed region with the highest probability using \"non-maximum suppression\" to removed redundant and overlapping bounding boxes and make that the final box.\n",
    "\n",
    "#### Note on DLIB library\n",
    "* Similar to the 6 step framework but uses the entire training image to get the P's (indicated by bounding boxes) and the N's (not containing bounding boxes).  Note: It looks like it is important that all of the objects are identified in the image. For example, when doing running bibs, I may ignore some bibs for some reasons (too small, partially blocked, too many). My guess is that these images should just simply be avoided. This technique eliminates steps 2, 4, and 5.\n",
    "* non-maximum supression is applied during the trainig phase helping to reduce false positives\n",
    "* dlib using a highly accurate SVM engine used to find the hyperplane separating the TWO classes.\n",
    "\n",
    "\n",
    "#### Use a JSON file to hold the hyper-parameters\n",
    "{\n",
    "\n",
    "\"faces_folder\": \"B:\\\\DataSets\\\\2016_USATF_Sprint_TrainingDataset\"\n",
    "\"myTrainingFilename\": \"trainingset_small.xml\"\n",
    "\"myTestingFilename: \"trainingset_small.xml\"\n",
    "\"myDetector\": \"detector.svm\"\n",
    "}\n",
    "\n",
    "#### Load and Dump hdf5 file\n",
    "* hdf5 provides efficient data storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from skimage import feature,exposure\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "import simplejson as json\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from scipy import io\n",
    "from lxml import etree\n",
    "import math\n",
    "import glob\n",
    "#from imutils import paths\n",
    "##import progressbar\n",
    "#import cv2\n",
    "#from pyimagesearch.object_detection import helpers\n",
    "# import the necessary packages\n",
    "# conda install -c anaconda simplejson\n",
    "#import commentjson as json\n",
    "#from pyimagesearch.object_detection import helpers\n",
    "#from pyimagesearch.descriptors import HOG\n",
    "#from pyimagesearch.utils import dataset\n",
    "#from pyimagesearch.utils import Conf\n",
    "##from imutils import paths\n",
    "##from imutils import resize\n",
    "#from scipy import io\n",
    "##import progressbar\n",
    "#import import_training_images_function2 as imp # Used to either import via Matlab file (CalTech) or XML (Scikit-learn)\n",
    "#from skimage import exposure\n",
    "#conda install -c anaconda progressbar\n",
    "#from dlib import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HOG:\n",
    "\tdef __init__(self, orientations=12, pixelsPerCell=(4, 4), cellsPerBlock=(2, 2), normalize=True):\n",
    "\t\t# store the number of orientations, pixels per cell, cells per block, and\n",
    "\t\t# whether normalization should be applied to the image\n",
    "\t\tself.orientations = orientations\n",
    "\t\tself.pixelsPerCell = pixelsPerCell\n",
    "\t\tself.cellsPerBlock = cellsPerBlock\n",
    "\t\tself.normalize = normalize\n",
    "\n",
    "\tdef describe(self, image):\n",
    "\t\t# compute Histogram of Oriented Gradients features\n",
    "\t\thist = feature.hog(image, orientations=self.orientations, pixels_per_cell=self.pixelsPerCell,cells_per_block=self.cellsPerBlock, transform_sqrt=self.normalize)\n",
    "\t\thist[hist < 0] = 0\n",
    "\n",
    "\t\t# return the histogram\n",
    "\t\treturn hist\n",
    "\n",
    "\tdef describe_and_return_HOGImage(self, image):\n",
    "\t\t# compute Histogram of Oriented Gradients features\n",
    "\t\t(hist,hogImage) = feature.hog(image, orientations=self.orientations, pixels_per_cell=self.pixelsPerCell,cells_per_block=self.cellsPerBlock, transform_sqrt=self.normalize, visualise=True)\n",
    "\t\thist[hist < 0] = 0\n",
    "\n",
    "\t\t# return the histogram\n",
    "\t\treturn hist,hogImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Referred to as helpers.py in PyImageSearch Class\n",
    "def crop_ct101_bb(image, bb, padding=10, dstSize=(32, 32)):\n",
    "\t# unpack the bounding box, extract the ROI from the image, while taking into account\n",
    "\t# the supplied offset\n",
    "\t(y, h, x, w) = bb # Looks like this is y1,y2,x1,x2\n",
    "\t#print(\"y,h,x,w ={} {} {} {}\".format(y,h,x,w))\n",
    "\t(x, y) = (max(x - padding, 0), max(y - padding, 0))\n",
    "\troi = image[y:h + padding, x:w + padding]\n",
    "\t#print(\"ROI: {}\".format(roi))\n",
    "\t# resize the ROI to the desired destination size\n",
    "\t# It is important to resize the roi in order to keep the final feature vector the same size\t\n",
    "\troi = cv2.resize(roi, dstSize, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\t# return the ROI\n",
    "\treturn roi\n",
    "\n",
    "def pyramid(image, scale=1.5, minSize=(30, 30)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\n",
    "\t# keep looping over the pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the new dimensions of the image and resize it\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\th = int(image.shape[0] / scale)        \n",
    "\t\t#image = imutils.resize(image, width=w)\n",
    "\t\timage = cv2.resize(image, (w,h))\n",
    "\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image\n",
    "\n",
    "def sliding_window(image, stepSize, windowSize):\n",
    "\t# slide a window across the image\n",
    "\tfor y in range(0, image.shape[0], stepSize):\n",
    "\t\tfor x in range(0, image.shape[1], stepSize):\n",
    "\t\t\t# yield the current window\n",
    "\t\t\t#print(\"X: {}\".format(x))\n",
    "\t\t\t#print(\"Y: {}\".format(y))\n",
    "\t\t\t#print(\"Window Shape Check: {}\".format(image.shape[:2]))\n",
    "\t\t\tyield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CalculateNumberOfScales(image,scale,minSize):\n",
    "    keepscale=[]\n",
    "    for i,layer in enumerate(pyramid(image, scale, minSize)):\n",
    "         keepscale.append(image.shape[0] / float(layer.shape[0]))\n",
    "    return keepscale   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process,Queue\n",
    "class ObjectDetector_multiprocessing:\n",
    "    def __init__(self, model, desc):\n",
    "        # store the classifier and HOG descriptor\n",
    "        self.model = model\n",
    "        self.desc = desc\n",
    "\n",
    "    def detect(self, q, image, winDim, winStep=4, pyramidScale=1.5, minProb=0.7,scale=1):\n",
    "        # initialize the list of bounding boxes and associated probabilities\n",
    "        boxes = []\n",
    "        probs = []\n",
    "        \n",
    "        # Calculate number of loops\n",
    "        #i=CalculateNumberOfScales(image,pyramidScale,winDim)         \n",
    "        #print(\"This image will use {} scales\".format(i))    \n",
    "        # loop over the image pyramid\n",
    "        for i,layer in enumerate(pyramid(image, scale=pyramidScale, minSize=winDim)):\n",
    "                       \n",
    "            \n",
    "            # determine the current scale of the pyramid\n",
    "            #scale = image.shape[0] / float(layer.shape[0])\n",
    "        \n",
    "            print(\"[INFO] Investigating layer {} at scale: {}\".format(i,scale))\n",
    "            # loop over the sliding windows for the current pyramid layer\n",
    "            counter =0\n",
    "            for (x, y, window) in sliding_window(layer, winStep, winDim):\n",
    "                # grab the dimensions of the window\n",
    "                (winH, winW) = window.shape[:2]\n",
    "                counter = counter+1\n",
    "                # ensure the window dimensions match the supplied sliding window dimensions\n",
    "                if winH == winDim[1] and winW == winDim[0]:\n",
    "                    # extract HOG features from the current window and classifiy whether or\n",
    "                    # not this window contains an object we are interested in\n",
    "                    #print(\"[INFO] Extracting HOG features\")\n",
    "                    features = self.desc.describe(window).reshape(1, -1)\n",
    "                    #print(\"Object Detector Feature Size: {}\".format(features.shape))\n",
    "                    prob = self.model.predict_proba(features)[0][1]\n",
    "                    if counter % 1000 ==0:\n",
    "                        print(\"[INFO] Model Probability: {}  Loop: {}   KeyPoint Top Left Corner (x,y) {}\".format(prob, counter,[x,y]))\n",
    "\n",
    "                    # check to see if the classifier has found an object with sufficient\n",
    "                    # probability\n",
    "                    if prob > minProb:\n",
    "                        ##print(\"[INFO] ********** Found a candidate! **************\")\n",
    "                        # compute the (x, y)-coordinates of the bounding box using the current\n",
    "                        # scale of the image pyramid\n",
    "                        (startX, startY) = (int(scale * x), int(scale * y))\n",
    "                        endX = int(startX + (scale * winW))\n",
    "                        endY = int(startY + (scale * winH))\n",
    "\n",
    "                        # update the list of bounding boxes and probabilities\n",
    "                        boxes.append((startX, startY, endX, endY))\n",
    "                        probs.append(prob)\n",
    "\n",
    "        # return a tuple of the bounding boxes and probabilities\n",
    "        q.put([boxes,probs])\n",
    "        return (boxes, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ObjectDetector:\n",
    "    def __init__(self, model, desc):\n",
    "        # store the classifier and HOG descriptor\n",
    "        self.model = model\n",
    "        self.desc = desc\n",
    "\n",
    "    def detect(self, image, winDim, winStep=4, pyramidScale=1.5, minProb=0.7):\n",
    "        # initialize the list of bounding boxes and associated probabilities\n",
    "        boxes = []\n",
    "        probs = []\n",
    "        \n",
    "        # Calculate number of loops\n",
    "        #i=CalculateNumberOfScales(image,pyramidScale,winDim)         \n",
    "        #print(\"This image will use {} scales\".format(i))    \n",
    "        # loop over the image pyramid\n",
    "        for i,layer in enumerate(pyramid(image, scale=pyramidScale, minSize=winDim)):\n",
    "                       \n",
    "            \n",
    "            # determine the current scale of the pyramid\n",
    "            scale = image.shape[0] / float(layer.shape[0])\n",
    "            print(\"[INFO] Investigating layer {} at scale: {}\".format(i,scale))\n",
    "            # loop over the sliding windows for the current pyramid layer\n",
    "            counter =0\n",
    "            for (x, y, window) in sliding_window(layer, winStep, winDim):\n",
    "                # grab the dimensions of the window\n",
    "                (winH, winW) = window.shape[:2]\n",
    "                counter = counter+1\n",
    "                # ensure the window dimensions match the supplied sliding window dimensions\n",
    "                if winH == winDim[1] and winW == winDim[0]:\n",
    "                    # extract HOG features from the current window and classifiy whether or\n",
    "                    # not this window contains an object we are interested in\n",
    "                    #print(\"[INFO] Extracting HOG features\")\n",
    "                    features = self.desc.describe(window).reshape(1, -1)\n",
    "                    #print(\"Object Detector Feature Size: {}\".format(features.shape))\n",
    "                    prob = self.model.predict_proba(features)[0][1]\n",
    "                    if counter % 1000 ==0:\n",
    "                        print(\"[INFO] Model Probability: {}  Loop: {}   KeyPoint Top Left Corner (x,y) {}\".format(prob, counter,[x,y]))\n",
    "\n",
    "                    # check to see if the classifier has found an object with sufficient\n",
    "                    # probability\n",
    "                    if prob > minProb:\n",
    "                        ##print(\"[INFO] ********** Found a candidate! **************\")\n",
    "                        # compute the (x, y)-coordinates of the bounding box using the current\n",
    "                        # scale of the image pyramid\n",
    "                        (startX, startY) = (int(scale * x), int(scale * y))\n",
    "                        endX = int(startX + (scale * winW))\n",
    "                        endY = int(startY + (scale * winH))\n",
    "\n",
    "                        # update the list of bounding boxes and probabilities\n",
    "                        boxes.append((startX, startY, endX, endY))\n",
    "                        probs.append(prob)\n",
    "\n",
    "        # return a tuple of the bounding boxes and probabilities\n",
    "        return (boxes, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, probs, overlapThresh):\n",
    "\t# if there are no boxes, return an empty list\n",
    "\tif len(boxes) == 0:\n",
    "\t\treturn []\n",
    "\n",
    "\t# if the bounding boxes are integers, convert them to floats -- this is important since\n",
    "\t# we'll be doing a bunch of divisions\n",
    "\tif boxes.dtype.kind == \"i\":\n",
    "\t\tboxes = boxes.astype(\"float\")\n",
    "\n",
    "\t# initialize the list of picked indexes\n",
    "\tpick = []\n",
    "\n",
    "\t# grab the coordinates of the bounding boxes\n",
    "\tx1 = boxes[:, 0]\n",
    "\ty1 = boxes[:, 1]\n",
    "\tx2 = boxes[:, 2]\n",
    "\ty2 = boxes[:, 3]\n",
    "\n",
    "\t# compute the area of the bounding boxes and sort the bounding boxes by their associated\n",
    "\t# probabilities\n",
    "\tarea = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\tidxs = np.argsort(probs)\n",
    "\n",
    "\t# keep looping while some indexes still remain in the indexes list\n",
    "\twhile len(idxs) > 0:\n",
    "\t\t# grab the last index in the indexes list and add the index value to the list of\n",
    "\t\t# picked indexes\n",
    "\t\tlast = len(idxs) - 1\n",
    "\t\ti = idxs[last]\n",
    "\t\tpick.append(i)\n",
    "\n",
    "\t\t# find the largest (x, y) coordinates for the start of the bounding box and the\n",
    "\t\t# smallest (x, y) coordinates for the end of the bounding box\n",
    "\t\txx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "\t\tyy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "\t\txx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "\t\tyy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "\t\t# compute the width and height of the bounding box\n",
    "\t\tw = np.maximum(0, xx2 - xx1 + 1)\n",
    "\t\th = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "\t\t# compute the ratio of overlap\n",
    "\t\toverlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "\t\t# delete all indexes from the index list that have overlap greater than the\n",
    "\t\t# provided overlap threshold\n",
    "\t\tidxs = np.delete(idxs, np.concatenate(([last],\n",
    "\t\t\tnp.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "\t# return only the bounding boxes that were picked\n",
    "\treturn boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_dataset(data, labels, path, datasetName, writeMethod=\"w\"):\n",
    "    # open the database, create the dataset, write the data and labels to dataset,\n",
    "    # and then close the database\n",
    "    with h5py.File(path, writeMethod) as db:\n",
    "        dataset = db.create_dataset(datasetName, (len(data), len(data[0]) + 1), dtype=\"float\")\n",
    "        dataset[0:len(data)] = np.c_[labels, data]\n",
    "        db.close()\n",
    "    print(\"Finished Dumping Data into: \".format(path))\n",
    "\n",
    "def load_dataset(path, datasetName):\n",
    "    # open the database, grab the labels and data, then close the dataset\n",
    "    with h5py.File(path, \"r\") as db:\n",
    "        (labels, data) = (db[datasetName][:, 0], db[datasetName][:, 1:])\n",
    "        db.close()\n",
    "\n",
    "    # return a tuple of the data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf:\n",
    "\tdef __init__(self, confPath):\n",
    "\t\t# load and store the configuration and update the object's dictionary\n",
    "\t\tconf = json.loads(open(confPath).read())\n",
    "\t\tself.__dict__.update(conf)\n",
    "\n",
    "\tdef __getitem__(self, k):\n",
    "\t\t# return the value associated with the supplied key\n",
    "\t\treturn self.__dict__.get(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_with_Matlab(conf,hog,SW):\n",
    "    data = []   \n",
    "    labels = []\n",
    "    # grab the set of ground-truth images and select a percentage of them for training\n",
    "    #trnPaths = list(paths.list_images(conf[\"image_dataset\"]))\n",
    "    trnPaths = list(os.listdir(conf[\"image_dataset\"]))\n",
    "    trnPaths = random.sample(trnPaths, int(len(trnPaths) * conf[\"percent_gt_images\"]))\n",
    "    print(\"[INFO] describing training ROIs...\")\n",
    "    # setup the progress bar\n",
    "    #widgets = [\"Extracting: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    #pbar = progressbar.ProgressBar(maxval=len(trnPaths), widgets=widgets).start()\n",
    "    # loop over the training paths\n",
    "    for (i, trnPath) in enumerate(trnPaths):\n",
    "        # load the image, convert it to grayscale, and extract the image ID from the path\n",
    "        image = cv2.imread(trnPath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        imageID = trnPath[trnPath.rfind(\"_\") + 1:].replace(\".jpg\", \"\")\n",
    "        \n",
    "        # load the annotation file associated with the image and extract the bounding box\n",
    "        p = \"{}/annotation_{}.mat\".format(conf[\"image_annotations\"], imageID)\n",
    "        bb = io.loadmat(p)[\"box_coord\"][0] #(y,h,x,w)\n",
    "        # The next line crops the image to only the object. Because of this, no scanning is required\n",
    "        # and the image size can simply be set to the scanning size (plus offset) so that only one scan is needed\n",
    "        #roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=tuple(conf[\"window_dim\"]))\n",
    "        roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=SW)\n",
    "        # define the list of ROIs that will be described, based on whether or not the\n",
    "        # horizontal flip of the image should be used\n",
    "        rois = (roi, cv2.flip(roi, 1)) if conf[\"use_flip\"] else (roi,)\n",
    "        \n",
    "        # loop over the ROIs\n",
    "        for roi in rois:\n",
    "        \t# extract features from the ROI and update the list of features and labels\n",
    "        \tfeatures = hog.describe(roi)\n",
    "        \tdata.append(features)\n",
    "        \tlabels.append(1)\n",
    "    \n",
    "        # update the progress bar\n",
    "        #\tpbar.update(i)\n",
    "    return  data,labels\n",
    "\n",
    "\n",
    "\n",
    "def import_from_XML(conf,hog,SW):\n",
    "    data = []   \n",
    "    labels = []\n",
    "    ##print(\"Importing: {}\".format(conf[\"image_dataset_XML\"])) \n",
    "    doc = etree.parse(conf[\"image_dataset_XML\"])\n",
    "    MyXML=doc.find('images')\n",
    "    ## widgets = [\"Extracting: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    ## pbar = progressbar.ProgressBar(maxval=len(doc.xpath(\".//*\")), widgets=widgets).start()\n",
    "    # loop over the training paths\n",
    "    #for (i, info) in enumerate(MyXML):\n",
    "    i = 0\n",
    "    for info in MyXML:\n",
    "        # load the image, convert it to grayscale, and extract the image ID from the path\n",
    "        try:\n",
    "            imagename=conf[\"image_dataset\"] + \"\\\\\" + info.get('file')\n",
    "            #print(\"Working on file: {}\".format(imagename))\n",
    "            image = cv2.imread(imagename)\n",
    "            #cv2.imshow(\"My Image\",image)\n",
    "            #cv2.waitKey(0)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            y=int(info[0].get('top'))\n",
    "            x=int(info[0].get('left'))\n",
    "            w=int(info[0].get('width'))\n",
    "            h=int(info[0].get('height')) \n",
    "            bb =[int(y),int(y)+int(h),int(x),int(x)+int(w)]  # [ y h x w] % (Look into h may be top and top/y may actually be h\n",
    "            #print(\"bb: {}\".format(bb))\n",
    "            #newimage=image[y:y+h,x:x+w]\n",
    "            #roi = crop_ct101_bb(image, bb, padding=conf[\"offset\"], dstSize=tuple(conf[\"window_dim\"]))\n",
    "            # The next line crops the image to only the object. Because of this, no scanning is required\n",
    "            # and the image size can simply be set to the scanning size (plus offset) so that only one scan is needed\n",
    "            #roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=tuple(conf[\"image_resized\"]))\n",
    "            roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=SW)\n",
    "            ##print(\"The image size is {}.\".format(roi.shape))\n",
    "            ##cv2.imshow(imagename,roi)\n",
    "            ##cv2.waitKey(0)         \n",
    "            # define the list of ROIs that will be described, based on whether or not the\n",
    "            # horizontal flip of the image should be used\n",
    "            if conf[\"use_flip\"]:\n",
    "                rois = (roi, cv2.flip(roi, 1))\n",
    "            else:\n",
    "                rois = (roi,)\n",
    "                        \n",
    "            # loop over the ROIs\n",
    "            for roi in rois:\n",
    "                # extract features from the ROI and update the list of features and labels\n",
    "                features = hog.describe(roi)\n",
    "                data.append(features)\n",
    "                labels.append(1)\n",
    "        \n",
    "                # update the progress bar\n",
    "                #pbar.update(i)\n",
    "        except:\n",
    "            print(\"Issue with file:  {}\".format(imagename))\n",
    "            \n",
    "        i=i+1\n",
    "        \n",
    "    return  data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetAvgDimensions(conf):\n",
    "    doc = etree.parse(conf[\"image_dataset_XML\"])\n",
    "    MyXML=doc.find('images')\n",
    "    widths = []\n",
    "    heights = []\n",
    "    i = 0\n",
    "    for info in MyXML:\n",
    "        imagename=conf[\"image_dataset\"] + \"\\\\\" + info.get('file')\n",
    "        image = cv2.imread(imagename)\n",
    "        #print(\"Reading {}\".format(imagename))\n",
    "        widths.append(int(info[0].get('width')))\n",
    "        heights.append(int(info[0].get('height')) )\n",
    "\n",
    "    (avgWidth, avgHeight) = (np.mean(widths), np.mean(heights))\n",
    "    (stdW,stdH)=(np.std(widths),np.std(heights))\n",
    "    #print(\"The length of widths is {}\".format(len(widths)))\n",
    "    newW=math.ceil(int(avgWidth/2)/4)*4\n",
    "    newH=math.ceil(int(avgHeight/2)/4)*4\n",
    "    print(\"[INFO] avg. width: {:.2f} +/- {:.2f}\".format(avgWidth,stdW))\n",
    "    print(\"[INFO] avg. height: {:.2f} +/- {:.2f}\".format(avgHeight,stdH))\n",
    "    print(\"[INFO] aspect ratio: {:.2f}\".format(avgWidth / avgHeight))\n",
    "    print(\"[INFO] The recommended Sliding Window Size is W:{}  H:{}\".format(newW,newH))\n",
    "    print(\"[INFO] Sliding Window Aspect Ratio {:.2f}\".format(newW / newH))\n",
    "    return tuple([newW,newH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TestThread(conf,SW):\n",
    "    pfile=conf[\"classifier_path\"]\n",
    "    with open(pfile, 'rb') as f:\n",
    "        model = cPickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    hog = HOG(orientations=conf[\"orientations\"], pixelsPerCell=tuple(conf[\"pixels_per_cell\"]),\n",
    "    cellsPerBlock=tuple(conf[\"cells_per_block\"]), normalize=conf[\"normalize\"])\n",
    "    od = ObjectDetector(model, hog)\n",
    "\n",
    "    # grab the set of distraction paths and randomly sample them\n",
    "    #dstPaths = list(paths.list_images(conf[\"image_distractions\"]))\n",
    "    dstPaths=glob.glob(conf[\"image_distractions\"] + \"\\\\*.jpg\")\n",
    "    dstPaths = random.sample(dstPaths, conf[\"hn_num_distraction_images\"])\n",
    "\n",
    "\n",
    "    # loop over the distraction paths\n",
    "    for (i, imagePath) in enumerate(dstPaths):\n",
    "        # load the image and convert it to grayscale\n",
    "        image = cv2.imread(imagePath)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        print(\"[INFO] Scanning file: {}\".format(imagePath))\n",
    "        # detect objects in the image\n",
    "        winStep=conf[\"hn_window_step\"]\n",
    "        pyramidScale=conf[\"hn_pyramid_scale\"]\n",
    "        minProb=conf[\"hn_min_probability\"]\n",
    "        (boxes, probs) = od.detect(gray, SW, winStep, pyramidScale, minProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image will use 7 scales\n",
      "Scales: [1.0, 1.5, 2.25, 3.377110694183865, 5.070422535211268, 7.627118644067797, 11.464968152866241]\n"
     ]
    }
   ],
   "source": [
    "imagePath=\"M:\\\\DataSets\\\\SprintPhotos_Small\\\\dsc_3332.jpg\"\n",
    "image = cv2.imread(imagePath)\n",
    "myscales=CalculateNumberOfScales(image,1.5,SW)         \n",
    "print(\"This image will use {} scales\".format(i)) \n",
    "#TestThread(conf,SW)\n",
    "print(\"Scales: {}\".format(myscales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Hard_Negative_Mining(conf,SW):\n",
    "    data = []\n",
    "    odlist=[]\n",
    "    # load the classifier, then initialize the Histogram of Oriented Gradients descriptor\n",
    "    # and the object detector\n",
    "    #model = cPickle.loads(open(conf[\"classifier_path\"]).read())-\n",
    "    pfile=conf[\"classifier_path\"]\n",
    "    with open(pfile, 'rb') as f:\n",
    "        model = cPickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    hog = HOG(orientations=conf[\"orientations\"], pixelsPerCell=tuple(conf[\"pixels_per_cell\"]),\n",
    "    cellsPerBlock=tuple(conf[\"cells_per_block\"]), normalize=conf[\"normalize\"])\n",
    "    \n",
    "    hnwinstep=conf[\"hn_window_step\"]\n",
    "    hnpyramidscale=conf[\"hn_pyramid_scale\"]\n",
    "    hnminprob= conf[\"hn_min_probability\"]\n",
    "    for i in range(0,11):\n",
    "        odlist.append(ObjectDetector_multiprocessing(model,hog)) # Create 10 objects to use\n",
    "    #od = ObjectDetector(model, hog)\n",
    "    #print(odlist)\n",
    "    # grab the set of distraction paths and randomly sample them\n",
    "    #dstPaths = list(paths.list_images(conf[\"image_distractions\"]))\n",
    "    dstPaths=glob.glob(conf[\"image_distractions\"] + \"\\\\*.jpg\")\n",
    "    dstPaths = random.sample(dstPaths, conf[\"hn_num_distraction_images\"])\n",
    "\n",
    "    # setup the progress bar\n",
    "    #widgets = [\"Mining: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    #pbar = progressbar.ProgressBar(maxval=len(dstPaths), widgets=widgets).start()\n",
    "\n",
    "    # loop over the distraction paths\n",
    "    for (i, imagePath) in enumerate(dstPaths):\n",
    "        # load the image and convert it to grayscale\n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        print(\"[INFO] Scanning file: {}\".format(imagePath))\n",
    "        # detect objects in the image\n",
    "        #(boxes, probs) = od.detect(gray, conf[\"window_dim\"], winStep=conf[\"hn_window_step\"],\n",
    "        #pyramidScale=conf[\"hn_pyramid_scale\"], minProb=conf[\"hn_min_probability\"])\n",
    "        myscales=CalculateNumberOfScales(image,1.5,SW)\n",
    "        q=[]\n",
    "        p=[]\n",
    "        d=[]\n",
    "        minp = min(len(myscales),3) # only 4 processors. Need to add code to handle additional layers\n",
    "        for j in range(0,minp):\n",
    "            #q.append(Queue())\n",
    "            q=Queue()\n",
    "            myod=odlist[j]\n",
    "            p.append(Process(target=myod.detect,args =(q,gray,SW,hnwinstep,hnpyramidscale,hnminprob,myscales[j])))\n",
    "        \n",
    "        print(p)\n",
    "        #print(p[0])\n",
    "        #print(p[1])\n",
    "        for j in range(0,minp):\n",
    "            pp=p[j]\n",
    "            print(pp)\n",
    "            pp.start()\n",
    "            pp.join()\n",
    "                #(boxes, probs) = od.detect(gray, SW, winStep=conf[\"hn_window_step\"],\n",
    "                #pyramidScale=conf[\"hn_pyramid_scale\"], minProb=conf[\"hn_min_probability\"])\n",
    "            \n",
    "        for j in range(0,minp):\n",
    "            d.append(q[j].get())\n",
    "            \n",
    "        # loop over the bounding boxes\n",
    "        minprob = conf[\"hn_min_probability\"]\n",
    "        print(\"[INFO] {} boxes were found with a probablility > {}\".format(len(boxes),conf[\"hn_min_probability\"]))\n",
    "        for (prob, (startX, startY, endX, endY)) in zip(probs, boxes):\n",
    "            # extract the ROI from the image, resize it to a known, canonical size, extract\n",
    "            # HOG features from teh ROI, and finally update the data\n",
    "            #roi = cv2.resize(gray[startY:endY, startX:endX], tuple(conf[\"window_dim\"]),interpolation=cv2.INTER_AREA)\n",
    "            roi = cv2.resize(gray[startY:endY, startX:endX],SW,interpolation=cv2.INTER_AREA)\n",
    "            features = hog.describe(roi)\n",
    "            ##data.append(np.hstack([[prob], features])) # This line gave errors\n",
    "            data.append(features)\n",
    "\n",
    "    labels = [-1] * len(data)\n",
    "    # update the progress bar\n",
    "    #pbar.update(i)\n",
    "\n",
    "    # sort the data points by confidence\n",
    "    #pbar.finish()\n",
    "    ###print(\"[INFO] sorting by probability...\")\n",
    "    ###data = np.array(data)\n",
    "    ###data = data[data[:, 0].argsort()[::-1]]\n",
    "\n",
    "    # dump the dataset to file\n",
    "    print(\"[INFO] dumping hard negatives to file...\")\n",
    "    #dump_dataset(data[:, 1:], [-1] * len(data), conf[\"features_path\"], \"hard_negatives\",writeMethod=\"a\")\n",
    "\n",
    "    if len(data)>0 :\n",
    "        dump_dataset(data, labels, conf[\"features_path\"], \"hard_negatives\",writeMethod=\"a\")\n",
    "    else: \n",
    "        print(\"No Hard-Negatives were found in the images provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin HOG Feature Extraction for Positive images and Negative images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myJSONFile = os.getcwd() + \"\\\\conf\\\\TrackBibs.json\"\n",
    "conf = Conf(myJSONFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] avg. width: 174.21 +/- 97.51\n",
      "[INFO] avg. height: 125.54 +/- 73.02\n",
      "[INFO] aspect ratio: 1.39\n",
      "[INFO] The recommended Sliding Window Size is W:88  H:64\n",
      "[INFO] Sliding Window Aspect Ratio 1.38\n"
     ]
    }
   ],
   "source": [
    "SW=GetAvgDimensions(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the HOG descriptor along with the list of data and labels\n",
    "hog = HOG(orientations=conf[\"orientations\"], pixelsPerCell=tuple(conf[\"pixels_per_cell\"]), \n",
    "          cellsPerBlock=tuple(conf[\"cells_per_block\"]), normalize=conf[\"normalize\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin HOG feature extraction of Positive Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Begin HOG Feature Extraction of Positive Images...\n",
      "Image XML File:\n",
      "M:\\DataSets\\2016_USATF_Sprint_TrainingDataset\\sprints_large.xml\n",
      "Image files are located in:\n",
      "M:\\DataSets\\2016_USATF_Sprint_TrainingDataset\n",
      "Using XML Format\n",
      "Finished\n",
      "There are 67 feature vectors and each vector contains 11340 elements for a total of 759780 elements.\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Begin HOG Feature Extraction of Positive Images...\")\n",
    "print(\"Image XML File:\")\n",
    "print(conf[\"image_dataset_XML\"])\n",
    "print(\"Image files are located in:\")\n",
    "print(conf[\"image_dataset\"])\n",
    "# Open dataset images and extract features for different scales. The extension will\n",
    "# determine how the bounding box information is presented (.XML (as used by DLIB), or .Mat (as used by CalTech))\n",
    "tmp=os.path.splitext(conf[\"image_dataset_XML\"]) #TODO  Need to come back to\n",
    "if tmp[1]==\".xml\":\n",
    "    print(\"Using XML Format\")\n",
    "# Run ExtractImageInfoFromXML_Hood.py\n",
    "    # Note: The sliding window size is calculated from the images. The value in the json file is bypassed\n",
    "    data,labels=import_from_XML(conf,hog,SW)\n",
    "else:\n",
    "# Run ExtractImageInfoFromMatlab_Hood.py\n",
    "    print(\"Using Matlab Format\")\n",
    "    data,labels=import_with_Matlab(conf,hog,SW)\n",
    "lenPositiveFeatures=len(data)\n",
    "print(\"Finished\")    \n",
    "print(\"There are {} feature vectors and each vector contains {} elements for a total of {} elements.\".format(len(data),len(data[0]),len(data) * len(data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin HOG feature extraction of Negative Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Begin HOG Feature Extraction of Negative Images...\n",
      "Finished\n",
      "There are now 117 feature vectors and each vector contains 11340 elements for a total of 1326780 elements.\n",
      "67 Positive features and 50 Negative features\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Begin HOG Feature Extraction of Negative Images...\")\n",
    "#dstPaths = list(os.listdir(conf[\"image_distractions\"]))\n",
    "dstPaths=glob.glob(conf[\"image_distractions\"] + \"\\\\*.jpg\")\n",
    "#dstPaths.append(glob.glob(conf[\"image_distractions2\"] + \"\\\\*.jpg\"))\n",
    "#dstPaths.append(glob.glob(conf[\"image_distractions3\"] + \"\\\\*.jpg\"))\n",
    "#dstPaths.append(glob.glob(conf[\"image_distractions4\"] + \"\\\\*.jpg\"))\n",
    "#print(files)\n",
    "\n",
    "#widgets = [\"Extracting: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "#pbar = progressbar.ProgressBar(maxval=conf[\"num_distraction_images\"], widgets=widgets).start()\n",
    "# loop over the desired number of distraction images\n",
    "patches=[]\n",
    "for i in np.arange(0, conf[\"num_distraction_images\"]):\n",
    "    # randomly select a distraction images, load it, convert it to grayscale, and\n",
    "    # then extract random pathces from the image\n",
    "    image = cv2.imread(random.choice(dstPaths))\n",
    "    ##image = resize(image,width=int(conf[\"max_image_width\"]))\n",
    "    image = cv2.resize(image,SW)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    patches.append(image)\n",
    "    # extract_patches_2d is a convienent ROI sampling implementation in scikit-learn\n",
    "    ##patches = extract_patches_2d(image, tuple(conf[\"window_dim\"]),max_patches=conf[\"num_distractions_per_image\"])\n",
    "    ##patches = extract_patches_2d(image, tuple(SW),max_patches=conf[\"num_distractions_per_image\"])\n",
    "# loop over the patches,\n",
    "for patch in patches:\n",
    "    # extract features from the patch, then update teh data and label list\n",
    "    ##features = hog.describe(patch)\n",
    "    features = hog.describe(patch)\n",
    "    data.append(features)\n",
    "    labels.append(-1)\n",
    "\n",
    "\n",
    "    # update the progress bar\n",
    "    #pbar.update(i)\n",
    "print(\"Finished\")    \n",
    "print(\"There are now {} feature vectors and each vector contains {} elements for a total of {} elements.\".format(len(data),len(data[0]),len(data) * len(data[0])))\n",
    "print(\"{} Positive features and {} Negative features\".format(lenPositiveFeatures,len(patches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dumping features and labels to file...\n",
      "Feature data is saved in M:\\DataSets\\2016_USATF_Sprint_TrainingDataset\\SprintBibPhotosLarge2.hdf5\n",
      "Finished Dumping Data into: \n"
     ]
    }
   ],
   "source": [
    "# dump the dataset to file\n",
    "#pbar.finish()\n",
    "print(\"[INFO] dumping features and labels to file...\")\n",
    "MyFeaturePath=conf[\"features_path\"]\n",
    "print(\"Feature data is saved in {}\".format(MyFeaturePath))\n",
    "dump_dataset(data, labels, MyFeaturePath, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training SVM using saved HOG feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'C' parameter for the SVM measures how 'strict' SVM is. Larger values indicate a tolerance for fewer mistakes. While this can lead to higher accuracy on the training data, it could lead to overfitting. Smaller values lead to a 'soft-classifier'. Initially, we let it make many mistakes knowing downstream, hard negative mining will help rectify many of these mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import LinearSVC \n",
    "import _pickle as cPickle\n",
    "def train_model(conf,useHardNegatives):\n",
    "    # load the configuration file and the initial dataset\n",
    "    print(\"[INFO] loading dataset...\")\n",
    "    #conf = Conf(args[\"conf\"])\n",
    "    (data, labels) = load_dataset(conf[\"features_path\"], \"features\") # contains images labled as good (+1) and bad (-1)\n",
    "    print(\"Example of a data point. Feature: {} and Label {}\".format(data[0],labels[0]))\n",
    "    if useHardNegatives > 0:\n",
    "        print(\"[INFO] loading hard negatives...\")\n",
    "        (hardData, hardLabels) = load_dataset(conf[\"features_path\"], \"hard_negatives\")\n",
    "        data = np.vstack([data, hardData])  # Combine data with hardData\n",
    "        labels = np.hstack([labels, hardLabels])\n",
    "\n",
    "    # Determine an optimal value for C\n",
    "    params = {\"C\": [.1, 1.0, 10.0, 100, 1000, 10000.0]}\n",
    "    ### Note: I cannot use params here. I would first have to extract the features for the\n",
    "    modeltemp = GridSearchCV(LinearSVC(random_state=42),params,cv=3)\n",
    "    modeltemp.fit(data,labels)\n",
    "    print(\"[INFO] best hyperparameters: {}\".format(modeltemp.best_params_))\n",
    "\n",
    "    # train the svd classifier\n",
    "    print(\"[INFO] training classifier...\")\n",
    "    model = SVC(kernel=\"linear\", C=conf[\"C\"], probability=True, random_state=42)\n",
    "    model.fit(data, labels)\n",
    "    print(classification_report(labels,model.predict(data)))\n",
    "\n",
    "    # dump the classifier to file\n",
    "    print(\"[INFO] dumping classifier...\")\n",
    "    myFileName=conf[\"classifier_path\"]\n",
    "    f = open(conf[\"classifier_path\"], \"wb\")\n",
    "    f.write(cPickle.dumps(model))\n",
    "    f.close()\n",
    "    print(\"Finished.  SVD is saved as: {}\".format(myFileName))\n",
    "\n",
    "def test_model(conf,image_Filename,SW):\n",
    "    # load the classifier, then initialize the Histogram of Oriented Gradients descriptor\n",
    "    # and the object detector\n",
    "    # load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_Filename)\n",
    "    #image = imutils.resize(image,width=int(conf[\"max_image_width\"]))\n",
    "    #image = imutils.resize(image,SW) # resizes to match size used during training\n",
    "    #image = cv2.resize(image, width=min(260, image.shape[1]))\n",
    "    minwidth=min(int(conf[\"max_image_width\"]), image.shape[0])\n",
    "    minheight=min(int(conf[\"max_image_width\"]), image.shape[1])\n",
    "    image = cv2.resize(image, (minwidth,minheight)) \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    #print(\"Image size: {}\".format(gray.shape))\n",
    "    pfile=conf[\"classifier_path\"]\n",
    "    with open(pfile, 'rb') as f:\n",
    "        model = cPickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "  \n",
    "    #hog = HOG(orientations=conf[\"orientations\"], pixelsPerCell=tuple(conf[\"pixels_per_cell\"]), \n",
    "    #      cellsPerBlock=tuple(conf[\"cells_per_block\"]), normalize=conf[\"normalize\"])\n",
    "    #print(hog)\n",
    "   \n",
    "    od = ObjectDetector(model, hog)\n",
    "\n",
    "    print(od)\n",
    "\n",
    "    # detect objects in the image and apply non-maxima suppression to the bounding boxes\n",
    "    print(\"Detecting the object\")\n",
    "    #winDim=conf[\"sliding_window_dim\"]\n",
    "    winDim=SW # Recall, the sliding window dimensions are computed.\n",
    "    winStep=conf[\"window_step\"]\n",
    "    pyramidScale=conf[\"pyramid_scale\"]\n",
    "    minProb=conf[\"min_probability\"]\n",
    "    (boxes, probs) = od.detect(gray,winDim,winStep,pyramidScale,minProb)\n",
    "    pick = non_max_suppression(np.array(boxes), probs, conf[\"overlap_thresh\"])\n",
    "    orig = image.copy()\n",
    "    print(\"Finished detecting the object\")  \n",
    "    ##print(\"boxes: {}\".format(boxes))\n",
    "\n",
    "    if len(boxes) <1 :\n",
    "        print(\"The object was not found\")\n",
    "    else:\n",
    "        # loop over the original bounding boxes and draw them\n",
    "        for (startX, startY, endX, endY) in boxes:\n",
    "            cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "\n",
    "    # loop over the allowed bounding boxes and draw them\n",
    "    for (startX, startY, endX, endY) in pick:\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "    # show the output images\n",
    "    cv2.imshow(\"Original\", orig)\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading dataset...\n",
      "Example of a data point. Feature: [ 0.03051709  0.00742838  0.04882491 ...,  0.00128204  0.          0.        ] and Label 1.0\n",
      "[INFO] best hyperparameters: {'C': 0.1}\n",
      "[INFO] training classifier...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.98      1.00      0.99        50\n",
      "        1.0       1.00      0.99      0.99        67\n",
      "\n",
      "avg / total       0.99      0.99      0.99       117\n",
      "\n",
      "[INFO] dumping classifier...\n",
      "Finished.  SVD is saved as: M:\\DataSets\\2016_USATF_Sprint_TrainingDataset\\svm_model2.cpickle\n"
     ]
    }
   ],
   "source": [
    "train_model(conf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Scanning file: M:\\DataSets\\Caltech256\\256_ObjectCategories\\013.birdbath\\013_0037.jpg\n",
      "[INFO] Investigating layer 0 at scale: 1.0\n",
      "[INFO] Investigating layer 1 at scale: 1.5034013605442176\n",
      "[INFO] Investigating layer 2 at scale: 2.2551020408163267\n",
      "[INFO] Scanning file: M:\\DataSets\\Caltech256\\256_ObjectCategories\\013.birdbath\\013_0043.jpg\n",
      "[INFO] Investigating layer 0 at scale: 1.0\n",
      "[INFO] Model Probability: 0.012716877452959931  Loop: 1000   KeyPoint Top Left Corner (x,y) [96, 52]\n",
      "[INFO] Model Probability: 0.04104680326634283  Loop: 2000   KeyPoint Top Left Corner (x,y) [196, 104]\n",
      "[INFO] Model Probability: 0.006142931585767425  Loop: 4000   KeyPoint Top Left Corner (x,y) [96, 212]\n",
      "[INFO] Investigating layer 1 at scale: 1.5023696682464456\n",
      "[INFO] Investigating layer 2 at scale: 2.2642857142857142\n",
      "[INFO] Investigating layer 3 at scale: 3.4086021505376345\n"
     ]
    }
   ],
   "source": [
    "TestThread(conf,SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Scanning file: M:\\DataSets\\Caltech256\\256_ObjectCategories\\013.birdbath\\013_0017.jpg\n",
      "[<Process(Process-26, initial)>, <Process(Process-27, initial)>, <Process(Process-28, initial)>]\n",
      "<Process(Process-26, initial)>\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-bb756d77b275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHard_Negative_Mining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-184-797eef0fd446>\u001b[0m in \u001b[0;36mHard_Negative_Mining\u001b[0;34m(conf, SW)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[1;31m#(boxes, probs) = od.detect(gray, SW, winStep=conf[\"hn_window_step\"],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AdrianSr\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AdrianSr\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AdrianSr\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AdrianSr\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AdrianSr\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "Hard_Negative_Mining(conf,SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model(conf,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(hardData, hardLabels) = load_dataset(conf[\"features_path\"], \"hard_negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hardData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Trained Model\n",
    "This step involves\n",
    "    1. Looping over all layers of the image pyramid.\n",
    "    2. Applying our sliding window at each layer of the pyramid.\n",
    "    3. Extracting HOG features from each window.\n",
    "    4. Passing the extracted HOG feature vectors to our model for classification.\n",
    "    5. Maintaining a list of bounding boxes that are reported to contain an object of interest with sufficient probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TestFile=\"M:\\\\DataSets\\\\SprintPhotos_Small\\\\dsc_4153_01.jpg\"\n",
    "TestFile=\"M:\\\\DataSets\\\\SprintPhotos_Small\\\\dsc_3332.jpg\"\n",
    "test_model(conf,TestFile,SW)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
