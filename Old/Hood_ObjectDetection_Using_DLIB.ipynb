{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  \n",
    "\n",
    "This example program shows how you can use dlib to make a HOG based object detector for things like faces, pedestrians, and any other semi-rigid object.  In particular, we go though the steps to train the kind of sliding window object detector first published by Dalal and Triggs in 2005 in the  paper Histograms of Oriented Gradients for Human Detection.\n",
    "\n",
    "It uses the DLIB Library \n",
    "(conda install -c conda-forge dlib=19.4)\n",
    "\n",
    "Note: I installed dlib in the \"dlib\" python environment and not the tf_nn environment because I saw a warning that some packages would have been downgraded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the XML files\n",
    "DLIB requires images and bounding boxes around the labelled object. It has its own strructure for the XML files:\n",
    "\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<dataset>\n",
    "    <name>dataset containing bounding box labels on images</name>\n",
    "    <comment>created by BBTag</comment>\n",
    "    <tags>\n",
    "        <tag name=\"RunBib\" color=\"#032585\"/>\n",
    "    </tags>\n",
    "    <images>\n",
    "        <image file=\"B:/DataSets/2016_USATF_Sprint_TrainingDataset/_hsp3997.jpg\">\n",
    "            <box top=\"945\" left=\"887\" width=\"85\" height=\"53\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "            <box top=\"971\" left=\"43\" width=\"103\" height=\"56\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "            <box top=\"919\" left=\"533\" width=\"100\" height=\"56\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "        </image>\n",
    "        <image file=\"B:/DataSets/2016_USATF_Sprint_TrainingDataset/_hsp3989.jpg\">\n",
    "            <box top=\"878\" left=\"513\" width=\"111\" height=\"62\">\n",
    "                <label>my_label</label>\n",
    "            </box>\n",
    "        </image>     \n",
    "   </images>\n",
    "</dataset>\n",
    "top: Top left y value\n",
    "height: Height (positive down)\n",
    "left: Top left x value\n",
    "width: Width (positive to the right)\n",
    "\n",
    "To create your own XML files you can use the imglab tool which can be found in the tools/imglab folder.  It is a simple graphical tool for labeling objects in images with boxes.  To see how to use it read the tools/imglab/README.txt file.  But for this example, we just use the training.xml file included with dlib.\n",
    "\n",
    "Its a two part process to load the tagger.\n",
    "1.) typing the following command:\n",
    "#####    b:\\HoodMachineLearning\\dlib\\tools\\build\\Release\\imglab.exe -c mydataset.xml B:\\HoodMachineLearning\\datasets\\MyImage\n",
    "2.) \n",
    "####     b:\\HoodMachineLearning\\dlib\\tools\\build\\Release\\imglab.exe -c mydataset.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pyramids and sliding windows\n",
    "\n",
    "The technique uses image pyramids and sliding windows to minimize the effect of object location and object size. The pyramid is a set of subsample images and the sliding window remains the same and moves from left to right and top to bottom of each scale of the image.\n",
    "\n",
    "### Image Pyramids\n",
    "<img src=\"ImagePyramid.jpg\">\n",
    "\n",
    "Note: Dalai and Triggs showed that performance is reduced if you apply gaussian smoothing at each layer==> ski this stip\n",
    "\n",
    "### Sliding Window\n",
    "\n",
    "<img src=\"sliding_window_example.gif\" loop=3>\n",
    "\n",
    "* It is common to use a stepSize of 4 to 8 pixels\n",
    "* windowSize is the size of the Kernal. An object detector will work best if the aspect ratio of the kernal is close to that of the desired object. Note: The sliding window size is also important for the HOG filter. For the HOG filter two parameters are important: <b>pixels_per_cell</b> and <b>cells_per_block </b>\n",
    "\n",
    "In order to avoid having to 'guess' at the best window size that will satisfy object detector requirements and HOG requirments, a \"explore_dims.py\" method is used.\n",
    "\n",
    "1.) Meet object detection requirments: loads all the images and computes the average width, average height, and computes the aspect ratio from those values.\n",
    "2.) Meet HOG requirments: Pyimage rule of thumb is to divide the above values by two (ie, 1/4th the average size)\n",
    "    * This reduces the size of the HOG feature vector\n",
    "    * By dividing by two, a nice balance is struck between HOG feature vector size and reasonable window size.\n",
    "    * Note: Our sliding_window dimension needs to be divisible by pixels_per_cell and cells_per_block so that the HOG descriptor will 'fit' into the window size\n",
    "    * Its common for 'pixels_per_cell' to be a multiple of 4 and cells_per_block in the set (1,2,3)\n",
    "    * Start with pixels_per_cell=(4,4) and cells_per_block=(2,2)\n",
    "    * For example, in the Pyimage example, average W: 184 and average H:62. Divide by 2 ==> 92,31\n",
    "    * Find values close to 92,31 that are divisible by 4 (and 2): 96,32  (Easy)\n",
    "    * OBSERVATION:  When defining the binding boxes, it is best if all are around the same size. This can be difficult.  \n",
    "\n",
    "### The 6 Step Framework\n",
    "1. Sample P positive samples for your training data of the objects you want to detect. Extract HOG features from these objects.\n",
    "    * If given an a general image containing the object, bounding boxes will also need to be given that indicate the location of the image\n",
    "2. Sample N negative samples that do not contain the object and extract HOG features. In general N>>P  (I'd suggest images similar in size and aspect ratio to the P samples. I'd also avoid the bounding boxes and make the entire image the negative image. Pyimagesearch recommends using the 13 Natural Scene Category of the vision.stanford.edu/resources_links.html page\n",
    "3. Train a Linear Support Vector Machine (SVM) on the negative images (class 0) and positive image (class 1)\n",
    "4. Hard Negative Mining - for the N negative images, apply Sliding window and test the classifier. Ideally, they should all return 0. If they return a 1 indicating an incorrect classification, add it to the training set (for the next round of re-training)\n",
    "5. Re-train classifier using with the added images from Hard Negative Mining (Usually once is enough)\n",
    "6. Apply against test dataset, define a box around regions of high probability, when finished with the image, find the boxed region with the highest probability using \"non-maximum suppression\" to removed redundant and overlapping bounding boxes and make that the final box.\n",
    "\n",
    "#### Note on DLIB library\n",
    "* Similar to the 6 step framework but uses the entire training image to get the P's (indicated by bounding boxes) and the N's (not containing bounding boxes).  Note: It looks like it is important that all of the objects are identified in the image. For example, when doing running bibs, I may ignore some bibs for some reasons (too small, partially blocked, too many). My guess is that these images should just simply be avoided. This technique eliminates steps 2, 4, and 5.\n",
    "* non-maximum supression is applied during the trainig phase helping to reduce false positives\n",
    "* dlib using a highly accurate SVM engine used to find the hyperplane separating the TWO classes.\n",
    "\n",
    "\n",
    "#### Use a JSON file to hold the hyper-parameters\n",
    "{\n",
    "\n",
    "\"faces_folder\": \"B:\\\\DataSets\\\\2016_USATF_Sprint_TrainingDataset\"\n",
    "\"myTrainingFilename\": \"trainingset_small.xml\"\n",
    "\"myTestingFilename: \"trainingset_small.xml\"\n",
    "\"myDetector\": \"detector.svm\"\n",
    "}\n",
    "\n",
    "#### Load and Dump hdf5 file\n",
    "* hdf5 provides efficient data storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faces_folder: Path to Main photos folder\n",
    "myTraining:  Path to Training Set of Photos\n",
    "myTesting:   Path to Testing Set (Not used for training, just for quantifying performance)\n",
    "myDetector:  I Think: Filename of the Model that is created and then used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faces_folder =\"B:\\\\DataSets\\\\2016_USATF_Sprint_TrainingDataset\"\n",
    "myTrainingFilename=\"trainingset_small.xml\"\n",
    "myTestingFilename=\"trainingset_small.xml\"\n",
    "myDetector=\"detector.svm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dlib object to simple object detections.\n",
    "     The train_simple_object_detector() function has a bunch of options, all of which come with reasonable default values.  The next few lines goes over some of these options.\n",
    "### Select the C Value\n",
    "    # The trainer is a kind of support vector machine and therefore has the usual\n",
    "    # SVM C parameter.  In general, a bigger C encourages it to fit the training\n",
    "    # data better but might lead to overfitting.  You must find the best C value\n",
    "    # empirically by checking how well the trained detector works on a test set of\n",
    "    # images you haven't trained on.  Don't just leave the value set at 5.  Try a\n",
    "    # few different C values and see what works best for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def train_object_detector(faces_folder,myTraining,myTesting,myDetector,myDetector2):\n",
    "options = dlib.simple_object_detector_training_options()\n",
    "options.add_left_right_image_flips = True\n",
    "options.C = 5\n",
    "# Tell the code how many CPU cores your computer has for the fastest training.\n",
    "# Note: DLIB does not use the GPU\n",
    "options.num_threads = 4 \n",
    "options.be_verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:\\DataSets\\2016_USATF_Sprint_TrainingDataset\\trainingset_small.xml\n",
      "B:\\DataSets\\2016_USATF_Sprint_TrainingDataset\\trainingset_small.xml\n"
     ]
    }
   ],
   "source": [
    "training_xml_path = os.path.join(faces_folder, myTrainingFilename)\n",
    "print(training_xml_path)\n",
    "testing_xml_path = os.path.join(faces_folder, myTestingFilename)\n",
    "print(testing_xml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training\n",
    "    This function does the actual training.  It will save the final detector to the file specified by myDetectorFileName.  The input is an XML file that lists the images in the training dataset and also contains the positions of the face boxes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nError! An impossible set of object boxes was given for training. All the boxes \nneed to have a similar aspect ratio and also not be smaller than about 400 \npixels in area. The following images contain invalid boxes: \n  B:/DataSets/2016_USATF_Sprint_TrainingDataset/dsc_4062.jpg\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f8ff7bbe41d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_simple_object_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_xml_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyDetector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: \nError! An impossible set of object boxes was given for training. All the boxes \nneed to have a similar aspect ratio and also not be smaller than about 400 \npixels in area. The following images contain invalid boxes: \n  B:/DataSets/2016_USATF_Sprint_TrainingDataset/dsc_4062.jpg\n"
     ]
    }
   ],
   "source": [
    "dlib.train_simple_object_detector(training_xml_path, myDetector, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\")  # Print blank line to create gap from previous output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Training accuracy: {}\".format(dlib.test_simple_object_detector(training_xml_path, myDetector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Testing accuracy: {}\".format(dlib.test_simple_object_detector(testing_xml_path, myDetector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
