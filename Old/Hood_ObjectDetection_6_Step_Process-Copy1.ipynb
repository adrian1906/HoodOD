{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  \n",
    "\n",
    "This example program develops a HOG based object detector for things like faces, pedestrians, and any other semi-rigid object.  In particular, we go though the steps to train the kind of sliding window object detector first published by Dalal and Triggs in 2005 in the  paper Histograms of Oriented Gradients for Human Detection.\n",
    "\n",
    "It is similar to the method implemented in dlib (more optimized). However, this technique allows more control of the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the XML files\n",
    "DLIB requires images and bounding boxes around the labelled object. It has its own strructure for the XML files:\n",
    "\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<dataset>\n",
    "    <name>dataset containing bounding box labels on images</name>\n",
    "    <comment>created by BBTag</comment>\n",
    "    <tags>\n",
    "        <tag name=\"RunBib\" color=\"#032585\"/>\n",
    "    </tags>\n",
    "    <images>\n",
    "        <image file=\"B:/DataSets/2016_USATF_Sprint_TrainingDataset/_hsp3997.jpg\">\n",
    "            <box top=\"945\" left=\"887\" width=\"85\" height=\"53\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "            <box top=\"971\" left=\"43\" width=\"103\" height=\"56\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "            <box top=\"919\" left=\"533\" width=\"100\" height=\"56\">\n",
    "                <label>RunBib</label>\n",
    "            </box>\n",
    "        </image>\n",
    "        <image file=\"B:/DataSets/2016_USATF_Sprint_TrainingDataset/_hsp3989.jpg\">\n",
    "            <box top=\"878\" left=\"513\" width=\"111\" height=\"62\">\n",
    "                <label>my_label</label>\n",
    "            </box>\n",
    "        </image>     \n",
    "   </images>\n",
    "</dataset>\n",
    "top: Top left y value\n",
    "height: Height (positive down)\n",
    "left: Top left x value\n",
    "width: Width (positive to the right)\n",
    "\n",
    "To create your own XML files you can use the imglab tool which can be found in the tools/imglab folder.  It is a simple graphical tool for labeling objects in images with boxes.  To see how to use it read the tools/imglab/README.txt file.  But for this example, we just use the training.xml file included with dlib.\n",
    "\n",
    "Its a two part process to load the tagger.\n",
    "1.) typing the following command:\n",
    "#####    b:\\HoodMachineLearning\\dlib\\tools\\build\\Release\\imglab.exe -c mydataset.xml B:\\HoodMachineLearning\\datasets\\MyImage\n",
    "2.) \n",
    "####     b:\\HoodMachineLearning\\dlib\\tools\\build\\Release\\imglab.exe -c mydataset.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pyramids and sliding windows\n",
    "\n",
    "The technique uses image pyramids and sliding windows to minimize the effect of object location and object size. The pyramid is a set of subsample images and the sliding window remains the same and moves from left to right and top to bottom of each scale of the image.\n",
    "\n",
    "### Image Pyramids\n",
    "<img src=\"ImagePyramid.jpg\">\n",
    "\n",
    "Note: Dalai and Triggs showed that performance is reduced if you apply gaussian smoothing at each layer==> ski this stip\n",
    "\n",
    "### Sliding Window\n",
    "\n",
    "<img src=\"sliding_window_example.gif\" loop=3>\n",
    "\n",
    "* It is common to use a stepSize of 4 to 8 pixels\n",
    "* windowSize is the size of the Kernal. An object detector will work best if the aspect ratio of the kernal is close to that of the desired object. Note: The sliding window size is also important for the HOG filter. For the HOG filter two parameters are important: <b>pixels_per_cell</b> and <b>cells_per_block </b>\n",
    "\n",
    "In order to avoid having to 'guess' at the best window size that will satisfy object detector requirements and HOG requirments, a \"explore_dims.py\" method is used.\n",
    "\n",
    "1.) Meet object detection requirments: loads all the images and computes the average width, average height, and computes the aspect ratio from those values.\n",
    "2.) Meet HOG requirments: Pyimage rule of thumb is to divide the above values by two (ie, 1/4th the average size)\n",
    "    * This reduces the size of the HOG feature vector\n",
    "    * By dividing by two, a nice balance is struck between HOG feature vector size and reasonable window size.\n",
    "    * Note: Our sliding_window dimension needs to be divisible by pixels_per_cell and cells_per_block so that the HOG descriptor will 'fit' into the window size\n",
    "    * Its common for 'pixels_per_cell' to be a multiple of 4 and cells_per_block in the set (1,2,3)\n",
    "    * Start with pixels_per_cell=(4,4) and cells_per_block=(2,2)\n",
    "    * For example, in the Pyimage example, average W: 184 and average H:62. Divide by 2 ==> 92,31\n",
    "    * Find values close to 92,31 that are divisible by 4 (and 2): 96,32  (Easy)\n",
    "    * OBSERVATION:  When defining the binding boxes, it is best if all are around the same size. This can be difficult.  \n",
    "\n",
    "### The 6 Step Framework\n",
    "1. Sample P positive samples for your training data of the objects you want to detect. Extract HOG features from these objects.\n",
    "    * If given an a general image containing the object, bounding boxes will also need to be given that indicate the location of the image\n",
    "2. Sample N negative samples that do not contain the object and extract HOG features. In general N>>P  (I'd suggest images similar in size and aspect ratio to the P samples. I'd also avoid the bounding boxes and make the entire image the negative image. Pyimagesearch recommends using the 13 Natural Scene Category of the vision.stanford.edu/resources_links.html page\n",
    "3. Train a Linear Support Vector Machine (SVM) on the negative images (class 0) and positive image (class 1)\n",
    "4. Hard Negative Mining - for the N negative images, apply Sliding window and test the classifier. Ideally, they should all return 0. If they return a 1 indicating an incorrect classification, add it to the training set (for the next round of re-training)\n",
    "5. Re-train classifier using with the added images from Hard Negative Mining (Usually once is enough)\n",
    "6. Apply against test dataset, define a box around regions of high probability, when finished with the image, find the boxed region with the highest probability using \"non-maximum suppression\" to removed redundant and overlapping bounding boxes and make that the final box.\n",
    "\n",
    "#### Note on DLIB library\n",
    "* Similar to the 6 step framework but uses the entire training image to get the P's (indicated by bounding boxes) and the N's (not containing bounding boxes).  Note: It looks like it is important that all of the objects are identified in the image. For example, when doing running bibs, I may ignore some bibs for some reasons (too small, partially blocked, too many). My guess is that these images should just simply be avoided. This technique eliminates steps 2, 4, and 5.\n",
    "* non-maximum supression is applied during the trainig phase helping to reduce false positives\n",
    "* dlib using a highly accurate SVM engine used to find the hyperplane separating the TWO classes.\n",
    "\n",
    "\n",
    "#### Use a JSON file to hold the hyper-parameters\n",
    "{\n",
    "\n",
    "\"faces_folder\": \"B:\\\\DataSets\\\\2016_USATF_Sprint_TrainingDataset\"\n",
    "\"myTrainingFilename\": \"trainingset_small.xml\"\n",
    "\"myTestingFilename: \"trainingset_small.xml\"\n",
    "\"myDetector\": \"detector.svm\"\n",
    "}\n",
    "\n",
    "#### Load and Dump hdf5 file\n",
    "* hdf5 provides efficient data storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "\n",
    "class HOG:\n",
    "\tdef __init__(self, orientations=12, pixelsPerCell=(4, 4), cellsPerBlock=(2, 2), normalize=True):\n",
    "\t\t# store the number of orientations, pixels per cell, cells per block, and\n",
    "\t\t# whether normalization should be applied to the image\n",
    "\t\tself.orientations = orientations\n",
    "\t\tself.pixelsPerCell = pixelsPerCell\n",
    "\t\tself.cellsPerBlock = cellsPerBlock\n",
    "\t\tself.normalize = normalize\n",
    "\n",
    "\tdef describe(self, image):\n",
    "\t\t# compute Histogram of Oriented Gradients features\n",
    "\t\thist = feature.hog(image, orientations=self.orientations, pixels_per_cell=self.pixelsPerCell,cells_per_block=self.cellsPerBlock, transform_sqrt=self.normalize)\n",
    "\t\thist[hist < 0] = 0\n",
    "\n",
    "\t\t# return the histogram\n",
    "\t\treturn hist\n",
    "\n",
    "\tdef describe_and_return_HOGImage(self, image):\n",
    "\t\t# compute Histogram of Oriented Gradients features\n",
    "\t\t(hist,hogImage) = feature.hog(image, orientations=self.orientations, pixels_per_cell=self.pixelsPerCell,cells_per_block=self.cellsPerBlock, transform_sqrt=self.normalize, visualise=True)\n",
    "\t\thist[hist < 0] = 0\n",
    "\n",
    "\t\t# return the histogram\n",
    "\t\treturn hist,hogImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def dump_dataset(data, labels, path, datasetName, writeMethod=\"w\"):\n",
    "    # open the database, create the dataset, write the data and labels to dataset,\n",
    "    # and then close the database\n",
    "    db = h5py.File(path, writeMethod)\n",
    "    dataset = db.create_dataset(datasetName, (len(data), len(data[0]) + 1), dtype=\"float\")\n",
    "    dataset[0:len(data)] = np.c_[labels, data]\n",
    "    db.close()\n",
    "    print(\"Finished Dumping Data\")\n",
    "\n",
    "def load_dataset(path, datasetName):\n",
    "    # open the database, grab the labels and data, then close the dataset\n",
    "    db = h5py.File(path, \"r\")\n",
    "    (labels, data) = (db[datasetName][:, 0], db[datasetName][:, 1:])\n",
    "    db.close()\n",
    "\n",
    "    # return a tuple of the data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# conda install -c anaconda simplejson\n",
    "#import commentjson as json\n",
    "import simplejson as json\n",
    "class Conf:\n",
    "\tdef __init__(self, confPath):\n",
    "\t\t# load and store the configuration and update the object's dictionary\n",
    "\t\tconf = json.loads(open(confPath).read())\n",
    "\t\tself.__dict__.update(conf)\n",
    "\n",
    "\tdef __getitem__(self, k):\n",
    "\t\t# return the value associated with the supplied key\n",
    "\t\treturn self.__dict__.get(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import imutils\n",
    "#import cv2\n",
    "\n",
    "def crop_ct101_bb(image, bb, padding=10, dstSize=(32, 32)):\n",
    "\t# unpack the bounding box, extract the ROI from the image, while taking into account\n",
    "\t# the supplied offset\n",
    "\t(y, h, x, w) = bb # Looks like this is y1,y2,x1,x2\n",
    "\t#print(\"y,h,x,w ={} {} {} {}\".format(y,h,x,w))\n",
    "\t(x, y) = (max(x - padding, 0), max(y - padding, 0))\n",
    "\troi = image[y:h + padding, x:w + padding]\n",
    "\t#print(\"ROI: {}\".format(roi))\n",
    "\t# resize the ROI to the desired destination size\n",
    "\t# It is important to resize the roi in order to keep the final feature vector the same size\t\n",
    "\troi = cv2.resize(roi, dstSize, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\t# return the ROI\n",
    "\treturn roi\n",
    "\n",
    "def pyramid(image, scale=1.5, minSize=(30, 30)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\n",
    "\t# keep looping over the pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the new dimensions of the image and resize it\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\timage = imutils.resize(image, width=w)\n",
    "\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image\n",
    "\n",
    "def sliding_window(image, stepSize, windowSize):\n",
    "\t# slide a window across the image\n",
    "\tfor y in xrange(0, image.shape[0], stepSize):\n",
    "\t\tfor x in xrange(0, image.shape[1], stepSize):\n",
    "\t\t\t# yield the current window\n",
    "\t\t\t#print(\"X: {}\".format(x))\n",
    "\t\t\t#print(\"Y: {}\".format(y))\n",
    "\t\t\t#print(\"Window Shape Check: {}\".format(image.shape[:2]))\n",
    "\t\t\tyield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#conda install -c anaconda progressbar\n",
    "#from dlib import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "#from pyimagesearch.object_detection import helpers\n",
    "#from pyimagesearch.descriptors import HOG\n",
    "#from pyimagesearch.utils import dataset\n",
    "#from pyimagesearch.utils import Conf\n",
    "##from imutils import paths\n",
    "##from imutils import resize\n",
    "#from scipy import io\n",
    "import numpy as np\n",
    "##import progressbar\n",
    "import argparse\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "#import import_training_images_function2 as imp # Used to either import via Matlab file (CalTech) or XML (Scikit-learn)\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "from lxml import etree\n",
    "#from imutils import paths\n",
    "##import progressbar\n",
    "#import cv2\n",
    "#from pyimagesearch.object_detection import helpers\n",
    "\n",
    "def import_with_Matlab(conf,hog,SW):\n",
    "    data = []   \n",
    "    labels = []\n",
    "    # grab the set of ground-truth images and select a percentage of them for training\n",
    "    #trnPaths = list(paths.list_images(conf[\"image_dataset\"]))\n",
    "    trnPaths = list(os.listdir(conf[\"image_dataset\"]))\n",
    "    trnPaths = random.sample(trnPaths, int(len(trnPaths) * conf[\"percent_gt_images\"]))\n",
    "    print(\"[INFO] describing training ROIs...\")\n",
    "    # setup the progress bar\n",
    "    #widgets = [\"Extracting: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    #pbar = progressbar.ProgressBar(maxval=len(trnPaths), widgets=widgets).start()\n",
    "    # loop over the training paths\n",
    "    for (i, trnPath) in enumerate(trnPaths):\n",
    "        # load the image, convert it to grayscale, and extract the image ID from the path\n",
    "        image = cv2.imread(trnPath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        imageID = trnPath[trnPath.rfind(\"_\") + 1:].replace(\".jpg\", \"\")\n",
    "        \n",
    "        # load the annotation file associated with the image and extract the bounding box\n",
    "        p = \"{}/annotation_{}.mat\".format(conf[\"image_annotations\"], imageID)\n",
    "        bb = io.loadmat(p)[\"box_coord\"][0] #(y,h,x,w)\n",
    "        # The next line crops the image to only the object. Because of this, no scanning is required\n",
    "        # and the image size can simply be set to the scanning size (plus offset) so that only one scan is needed\n",
    "        #roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=tuple(conf[\"window_dim\"]))\n",
    "        roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=SW)\n",
    "        # define the list of ROIs that will be described, based on whether or not the\n",
    "        # horizontal flip of the image should be used\n",
    "        rois = (roi, cv2.flip(roi, 1)) if conf[\"use_flip\"] else (roi,)\n",
    "        \n",
    "        # loop over the ROIs\n",
    "        for roi in rois:\n",
    "        \t# extract features from the ROI and update the list of features and labels\n",
    "        \tfeatures = hog.describe(roi)\n",
    "        \tdata.append(features)\n",
    "        \tlabels.append(1)\n",
    "    \n",
    "        # update the progress bar\n",
    "        #\tpbar.update(i)\n",
    "    return  data,labels\n",
    "\n",
    "\n",
    "\n",
    "def import_from_XML(conf,hog,SW):\n",
    "    data = []   \n",
    "    labels = []\n",
    "    ##print(\"Importing: {}\".format(conf[\"image_dataset_XML\"])) \n",
    "    doc = etree.parse(conf[\"image_dataset_XML\"])\n",
    "    MyXML=doc.find('images')\n",
    "    ## widgets = [\"Extracting: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    ## pbar = progressbar.ProgressBar(maxval=len(doc.xpath(\".//*\")), widgets=widgets).start()\n",
    "    # loop over the training paths\n",
    "    #for (i, info) in enumerate(MyXML):\n",
    "    i = 0\n",
    "    for info in MyXML:\n",
    "        # load the image, convert it to grayscale, and extract the image ID from the path\n",
    "        imagename=conf[\"image_dataset\"] + \"\\\\\" + info.get('file')\n",
    "        #print(\"Working on file: {}\".format(imagename))\n",
    "        image = cv2.imread(imagename)\n",
    "        #cv2.imshow(\"My Image\",image)\n",
    "        #cv2.waitKey(0)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        y=int(info[0].get('top'))\n",
    "        x=int(info[0].get('left'))\n",
    "        w=int(info[0].get('width'))\n",
    "        h=int(info[0].get('height')) \n",
    "        bb =[int(y),int(y)+int(h),int(x),int(x)+int(w)]  # [ y h x w] % (Look into h may be top and top/y may actually be h\n",
    "        #print(\"bb: {}\".format(bb))\n",
    "        #newimage=image[y:y+h,x:x+w]\n",
    "        #roi = crop_ct101_bb(image, bb, padding=conf[\"offset\"], dstSize=tuple(conf[\"window_dim\"]))\n",
    "        # The next line crops the image to only the object. Because of this, no scanning is required\n",
    "        # and the image size can simply be set to the scanning size (plus offset) so that only one scan is needed\n",
    "        #roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=tuple(conf[\"image_resized\"]))\n",
    "        roi = crop_ct101_bb(image, bb, padding=conf[\"offset_padding\"], dstSize=SW)\n",
    "        ##print(\"The image size is {}.\".format(roi.shape))\n",
    "        ##cv2.imshow(imagename,roi)\n",
    "        ##cv2.waitKey(0)         \n",
    "        # define the list of ROIs that will be described, based on whether or not the\n",
    "        # horizontal flip of the image should be used\n",
    "        if conf[\"use_flip\"]:\n",
    "            rois = (roi, cv2.flip(roi, 1))\n",
    "        else:\n",
    "            rois = (roi,)\n",
    "                        \n",
    "        # loop over the ROIs\n",
    "        for roi in rois:\n",
    "        \t# extract features from the ROI and update the list of features and labels\n",
    "        \tfeatures = hog.describe(roi)\n",
    "        \tdata.append(features)\n",
    "        \tlabels.append(1)\n",
    "        \n",
    "        \t# update the progress bar\n",
    "        \t#pbar.update(i)\n",
    "        i=i+1\n",
    "    return  data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\\DataSets\\SprintPhotos_Small\n",
      "[]\n",
      "[INFO] avg. width: nan\n",
      "[INFO] avg. height: nan\n",
      "[INFO] aspect ratio: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdrianSr\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "C:\\Users\\AdrianSr\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "widths = []\n",
    "heights = []\n",
    "print(conf[\"image_annotations\"])\n",
    "p=glob.glob(conf[\"image_annotations\"] + \"/*.mat\")\n",
    "print(p)\n",
    "# loop over all annotations paths\n",
    "for p in glob.glob(conf[\"image_annotations\"] + \"/*.mat\"):\n",
    "\t# load the bounding box associated with the path and update the width and height\n",
    "\t# lists\n",
    "    (y, h, x, w) = io.loadmat(p)[\"box_coord\"][0]\n",
    "    widths.append(w - x)\n",
    "    heights.append(h - y)\n",
    "    print(y)\n",
    "\n",
    "# compute the average of both the width and height lists\n",
    "(avgWidth, avgHeight) = (np.mean(widths), np.mean(heights))\n",
    "print(\"[INFO] avg. width: {:.2f}\".format(avgWidth))\n",
    "print(\"[INFO] avg. height: {:.2f}\".format(avgHeight))\n",
    "print(\"[INFO] aspect ratio: {:.2f}\".format(avgWidth / avgHeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def GetAvgDimensions(conf):\n",
    "    doc = etree.parse(conf[\"image_dataset_XML\"])\n",
    "    MyXML=doc.find('images')\n",
    "    widths = []\n",
    "    heights = []\n",
    "    i = 0\n",
    "    for info in MyXML:\n",
    "        imagename=conf[\"image_dataset\"] + \"\\\\\" + info.get('file')\n",
    "        image = cv2.imread(imagename)\n",
    "        #print(\"Reading {}\".format(imagename))\n",
    "        widths.append(int(info[0].get('width')))\n",
    "        heights.append(int(info[0].get('height')) )\n",
    "\n",
    "    (avgWidth, avgHeight) = (np.mean(widths), np.mean(heights))\n",
    "    (stdW,stdH)=(np.std(widths),np.std(heights))\n",
    "    #print(\"The length of widths is {}\".format(len(widths)))\n",
    "    newW=math.ceil(int(avgWidth/2)/4)*4\n",
    "    newH=math.ceil(int(avgHeight/2)/4)*4\n",
    "    print(\"[INFO] avg. width: {:.2f} +/- {:.2f}\".format(avgWidth,stdW))\n",
    "    print(\"[INFO] avg. height: {:.2f} +/- {:.2f}\".format(avgHeight,stdH))\n",
    "    print(\"[INFO] aspect ratio: {:.2f}\".format(avgWidth / avgHeight))\n",
    "    print(\"[INFO] The recommended Sliding Window Size is W:{}  H:{}\".format(newW,newH))\n",
    "    print(\"[INFO] Sliding Window Aspect Ratio {:.2f}\".format(newW / newH))\n",
    "    return tuple([newW,newH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myJSONFile = os.getcwd() + \"\\\\conf\\\\TrackBibs.json\"\n",
    "conf = Conf(myJSONFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] avg. width: 304.14 +/- 106.22\n",
      "[INFO] avg. height: 247.05 +/- 91.45\n",
      "[INFO] aspect ratio: 1.23\n",
      "[INFO] The recommended Sliding Window Size is W:152  H:124\n",
      "[INFO] Sliding Window Aspect Ratio 1.23\n"
     ]
    }
   ],
   "source": [
    "SW=GetAvgDimensions(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize the HOG descriptor along with the list of data and labels\n",
    "hog = HOG(orientations=conf[\"orientations\"], pixelsPerCell=tuple(conf[\"pixels_per_cell\"]), \n",
    "          cellsPerBlock=tuple(conf[\"cells_per_block\"]), normalize=conf[\"normalize\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Begin Training...\n",
      "Image XML File:\n",
      "M:\\DataSets\\SprintPhotos_Small\\sprints.xml\n",
      "Image files are located in:\n",
      "M:\\DataSets\\SprintPhotos_Small\n",
      "Using XML Format\n",
      "Finished\n",
      "There are 21 feature vectors and each vector contains 39960 elements for a total of 839160 elements.\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Begin Training...\")\n",
    "print(\"Image XML File:\")\n",
    "print(conf[\"image_dataset_XML\"])\n",
    "print(\"Image files are located in:\")\n",
    "print(conf[\"image_dataset\"])\n",
    "# Open dataset images and extract features for different scales. The extension will\n",
    "# determine how the bounding box information is presented (.XML (as used by DLIB), or .Mat (as used by CalTech))\n",
    "tmp=os.path.splitext(conf[\"image_dataset_XML\"]) #TODO  Need to come back to\n",
    "if tmp[1]==\".xml\":\n",
    "    print(\"Using XML Format\")\n",
    "# Run ExtractImageInfoFromXML_Hood.py\n",
    "    # Note: The sliding window size is calculated from the images. The value in the json file is bypassed\n",
    "    data,labels=import_from_XML(conf,hog,SW)\n",
    "else:\n",
    "# Run ExtractImageInfoFromMatlab_Hood.py\n",
    "    print(\"Using Matlab Format\")\n",
    "    data,labels=import_with_Matlab(conf,hog,SW)\n",
    "lenPositiveFeatures=len(data)\n",
    "print(\"Finished\")    \n",
    "print(\"There are {} feature vectors and each vector contains {} elements for a total of {} elements.\".format(len(data),len(data[0]),len(data) * len(data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training Images that Do Not Contain the Object (Negative Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] describing distraction ROIs...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] describing distraction ROIs...\n",
      "Finished\n",
      "There are now 121 feature vectors and each vector contains 39960 elements for a total of 4835160 elements.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(\"[INFO] describing distraction ROIs...\")\n",
    "#dstPaths = list(os.listdir(conf[\"image_distractions\"]))\n",
    "dstPaths=glob.glob(conf[\"image_distractions\"] + \"\\\\*.jpg\")\n",
    "#print(files)\n",
    "\n",
    "#widgets = [\"Extracting: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "#pbar = progressbar.ProgressBar(maxval=conf[\"num_distraction_images\"], widgets=widgets).start()\n",
    "# loop over the desired number of distraction images\n",
    "patches=[]\n",
    "for i in np.arange(0, conf[\"num_distraction_images\"]):\n",
    "    # randomly select a distraction images, load it, convert it to grayscale, and\n",
    "    # then extract random pathces from the image\n",
    "    image = cv2.imread(random.choice(dstPaths))\n",
    "    ##image = resize(image,width=int(conf[\"max_image_width\"]))\n",
    "    image = cv2.resize(image,SW)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    patches.append(image)\n",
    "    # extract_patches_2d is a convienent ROI sampling implementation in scikit-learn\n",
    "    ##patches = extract_patches_2d(image, tuple(conf[\"window_dim\"]),max_patches=conf[\"num_distractions_per_image\"])\n",
    "    ##patches = extract_patches_2d(image, tuple(SW),max_patches=conf[\"num_distractions_per_image\"])\n",
    "# loop over the patches,\n",
    "for patch in patches:\n",
    "    # extract features from the patch, then update teh data and label list\n",
    "    ##features = hog.describe(patch)\n",
    "    features = hog.describe(patch)\n",
    "    data.append(features)\n",
    "    labels.append(-1)\n",
    "\n",
    "\n",
    "    # update the progress bar\n",
    "    #pbar.update(i)\n",
    "print(\"Finished\")    \n",
    "print(\"There are now {} feature vectors and each vector contains {} elements for a total of {} elements.\".format(len(data),len(data[0]),len(data) * len(data[0])))\n",
    "print(\"{} Positive features and {} Negative features\".format(lenPositiveFeatures),len(patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dumping features and labels to file...\n",
      "Finished Dumping Data\n"
     ]
    }
   ],
   "source": [
    "# dump the dataset to file\n",
    "#pbar.finish()\n",
    "print(\"[INFO] dumping features and labels to file...\")\n",
    "dump_dataset(data, labels, conf[\"features_path\"], \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faces_folder: Path to Main photos folder\n",
    "myTraining:  Path to Training Set of Photos\n",
    "myTesting:   Path to Testing Set (Not used for training, just for quantifying performance)\n",
    "myDetector:  I Think: Filename of the Model that is created and then used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faces_folder =\"B:\\\\DataSets\\\\2016_USATF_Sprint_TrainingDataset\"\n",
    "myTrainingFilename=\"trainingset_small.xml\"\n",
    "myTestingFilename=\"trainingset_small.xml\"\n",
    "myDetector=\"detector.svm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dlib object to simple object detections.\n",
    "     The train_simple_object_detector() function has a bunch of options, all of which come with reasonable default values.  The next few lines goes over some of these options.\n",
    "### Select the C Value\n",
    "    # The trainer is a kind of support vector machine and therefore has the usual\n",
    "    # SVM C parameter.  In general, a bigger C encourages it to fit the training\n",
    "    # data better but might lead to overfitting.  You must find the best C value\n",
    "    # empirically by checking how well the trained detector works on a test set of\n",
    "    # images you haven't trained on.  Don't just leave the value set at 5.  Try a\n",
    "    # few different C values and see what works best for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def train_object_detector(faces_folder,myTraining,myTesting,myDetector,myDetector2):\n",
    "options = dlib.simple_object_detector_training_options()\n",
    "options.add_left_right_image_flips = True\n",
    "options.C = 5\n",
    "# Tell the code how many CPU cores your computer has for the fastest training.\n",
    "# Note: DLIB does not use the GPU\n",
    "options.num_threads = 4 \n",
    "options.be_verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:\\DataSets\\2016_USATF_Sprint_TrainingDataset\\trainingset_small.xml\n",
      "B:\\DataSets\\2016_USATF_Sprint_TrainingDataset\\trainingset_small.xml\n"
     ]
    }
   ],
   "source": [
    "training_xml_path = os.path.join(faces_folder, myTrainingFilename)\n",
    "print(training_xml_path)\n",
    "testing_xml_path = os.path.join(faces_folder, myTestingFilename)\n",
    "print(testing_xml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training\n",
    "    This function does the actual training.  It will save the final detector to the file specified by myDetectorFileName.  The input is an XML file that lists the images in the training dataset and also contains the positions of the face boxes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nError! An impossible set of object boxes was given for training. All the boxes \nneed to have a similar aspect ratio and also not be smaller than about 400 \npixels in area. The following images contain invalid boxes: \n  B:/DataSets/2016_USATF_Sprint_TrainingDataset/dsc_4062.jpg\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f8ff7bbe41d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_simple_object_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_xml_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyDetector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: \nError! An impossible set of object boxes was given for training. All the boxes \nneed to have a similar aspect ratio and also not be smaller than about 400 \npixels in area. The following images contain invalid boxes: \n  B:/DataSets/2016_USATF_Sprint_TrainingDataset/dsc_4062.jpg\n"
     ]
    }
   ],
   "source": [
    "dlib.train_simple_object_detector(training_xml_path, myDetector, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\")  # Print blank line to create gap from previous output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Training accuracy: {}\".format(dlib.test_simple_object_detector(training_xml_path, myDetector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Testing accuracy: {}\".format(dlib.test_simple_object_detector(testing_xml_path, myDetector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
